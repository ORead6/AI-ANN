import numpy as np
import random
import matplotlib.pyplot as mp

#Change to get dataset in format as shown
testData = np.matrix([[10.4,4.393,9.291,0,0,0,4], 
            [9.95,4.239,8.622,0,0,0.8,0],
            [9.46,4.124,8.057,0,0,0.8,0],
            [9.41,4.363,7.925,2.4,24.8,0.8,61.6],
            [26.3,11.962,58.704,11.2,5.6,33.6,111.2],
            [32.1,10.237,34.416,0,0,1.6,0.8]])

#Changing between 1-0

def normaliseData(data, case):
#Assign realistic min and max values with the ranges of data
    maxVal = 100 #Look through data set and change this value appropriately
    minVal = 0

    #Normalise data set between 0-1 for compairson
    if (case == "pre"):
        for x in range(0, len(data)):
            data[x] = (data[x] - minVal) / (maxVal - minVal)

        data = np.matrix(data).transpose()
        return data
    
    #Taking data between 0-1 and changing to put through correct range of values
    elif (case == "post"):
        for x in range(0, len(data)):
            data[x] = (data[x] * (maxVal - minVal)) + minVal
        data = np.matrix(data).transpose()
        return data


#factor to shrink inputs by to avoid activation of 6 being nearly the same and as accurate as 600 (sigmoid graph)
inputFactor = 0

#Neurons is how long the test Data array is
I_dim = 7

#5 Neurons
H_dim = 5

#Learning Rate
global learning_param
learning_param = 0.1

#3D Array for weights per layer
global weights
weights = np.zeros(shape=(H_dim, len(testData)+1))

#Hidden Layer Bias
hB = np.zeros(H_dim)

#Last layer of weights:
global hW
hW = np.zeros(H_dim).transpose()

#output Bias
oB = 0

#Error Values
errorGraph = []

#Amount of Epochs to do
epochCount = 1

def initWeights():
    #Init weight values using Xavier weight Intialisation
    for x in range(0, len(weights)):
        for i in range(0, len(weights[x])):
            w = random.uniform(-(1/np.sqrt(I_dim)), (1/np.sqrt(I_dim)))
            weights[x][i] = w

    for i in range(0, len(hW)):
        w = random.uniform(-(1/np.sqrt(I_dim)), (1/np.sqrt(I_dim)))
        hW[i] = w

#Activation Function
def activation(x):
    active = "sigmoid"

    if active == "sigmoid":
        #Sigmoid
        result = 1 / (1 + np.exp(-x))
        return result
    
    if active == "RELU":
        #relu
        if (x) >= 0:
            return x
        else:
            return 0    

#Error Function
def costFunction(real, AI):
    real = normaliseData(real, "pre")
    for i in range(0, len(real)):
        error = abs(real[i] - AI[i])
        errorGraph.append(error.item(0))


#Apply weights to each input value for each hidden layer and get their values
#Loop through data set passing each data value into this
def feedForward(input_layer):
    global newA
    newA = np.zeros(H_dim)

    for x in range(len(weights)):
        #Weighted Sum
        a = np.matmul(weights[x], input_layer.transpose())

        #Add the biases
        a = np.add(a, hB[x])

        #Send value through activation for hidden node value
        a = activation(a[0])

        newA[x] = a

    outputNode = np.matmul(hW.transpose(), newA)

    return (float(outputNode + oB))

def derivative(output):
<<<<<<< HEAD
=======
    #Output is val of sigmoid(x)
>>>>>>> e4883047653bb648881975f41d0b168cacbd169c
    return (output * (1 - output))

def backProp(end_epoch, realVals):
    epoch_errors = np.zeros(len(end_epoch))
    realVals = normaliseData(realVals, "pre")

    for i in range(0, len(end_epoch)):
        error = (end_epoch[i] - realVals[i]) * derivative(end_epoch[i])
        epoch_errors[i] = error

    hidden_errors = [[0] * H_dim for each in end_epoch]
    for i in range(0, len(end_epoch)):
        for j in range(0, H_dim):
            output_error = epoch_errors[i]
            output = end_epoch[i]
            weight = hW[j]
            hidden_err = (weight * output_error) * derivative(output)
            hidden_errors[i][j] = hidden_err

<<<<<<< HEAD
    hidden_errors = np.matrix(hidden_errors)
    print("Errors on the OUTPUT nodes:" + str(epoch_errors))
    print("Errors on the HIDDEN nodes:" + str(hidden_errors))
=======
    #print("Errors on the OUTPUT nodes:" + str(epoch_errors))
    #print("Errors on the HIDDEN nodes:" + str(hidden_errors))


    for i in range(0, len(hidden_errors)-1):
        delta = epoch_errors[i]
        hW[i] += (learning_param * delta * hW[i])
        oB += (learning_param * delta)

    for i in range(0 , len(hidden_errors)-1):
        for j in range(0, len(hidden_errors[i])):
            delta = hidden_errors[i][j]
            weights[i][j] += (learning_param * delta * testData.item(i, j))
>>>>>>> e4883047653bb648881975f41d0b168cacbd169c

def realOutput():
    #Search through test data, and get the real outputs
    #Test data for now
    return [26.1, 24.86, 23.6, 23.47, 60.7, 98.01]

<<<<<<< HEAD
=======
def plotGraph():
    print(errorGraph)
    mp.plot([x for x in range(len(errorGraph))], errorGraph)
    mp.xlabel("Epochs")
    mp.ylabel("Error")
    mp.title("Error Over Time")
    mp.show()

>>>>>>> e4883047653bb648881975f41d0b168cacbd169c
def main():
    #Matrix to store values after each epoch
    initWeights()
    epochM = np.zeros(len(testData))
    for i in range(epochCount):


<<<<<<< HEAD

    #Get the desiredOutputs
    desiredOut = realOutput()

    backProp(epochM, desiredOut)


    
main()
=======
        #Send each layer of test Data through feed forward algorithm
        for x in range(0, len(testData)):
            epochM[x] = activation(feedForward(testData[x]))
>>>>>>> e4883047653bb648881975f41d0b168cacbd169c


        #Get the desiredOutputs
        desiredOut = realOutput()

        costFunction(desiredOut, epochM)
        backProp(epochM, desiredOut)

    plotGraph()
    
main()






