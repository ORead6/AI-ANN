import numpy as np

#Change to get dataset in format as shown
testData = np.matrix([[10.4,4.393,9.291,0,0,0,4], 
            [9.95,4.239,8.622,0,0,0.8,0],
            [9.46,4.124,8.057,0,0,0.8,0],
            [9.41,4.363,7.925,2.4,24.8,0.8,61.6],
            [26.3,11.962,58.704,11.2,5.6,33.6,111.2],
            [32.1,10.237,34.416,0,0,1.6,0.8]])

desiredOut = np.matrix([26.1, 24.86, 23.6, 23.47, 60.7, 98.01]).transpose()

#factor to shrink inputs by to avoid activation of 6 being nearly the same and as accurate as 600 (sigmoid graph)
inputFactor = 0

#Neurons is how long the test Data array is
I_dim = 7

#5 Neurons
H_dim = 5

#3D Array for weights per layer
weights = np.matrix([([0]*I_dim) for i in range(H_dim)])

#Hidden Layer Bias
hB = [1,2,3,4,5] #np.matrx([0] * H_dim)

#Last layer of weights:
hW = np.matrix(([0] * H_dim)).transpose()

#output Bias
oB = 0

#Activation Function
def activation(x):
    active = "RELU"

    if active == "sigmoid":
        #Sigmoid
        result = 1 / (1 + np.exp(x))
        return result
    
    if active == "RELU":
        #relu
        if (x) >= 0:
            return x
        else:
            return 0

#Error Function
def costFunction(real, AI):
    topSum = 0
    bottomSum = 0
    #Coefficient of efficiency
    realMean = real.sum() / len(real)
    for i in range(len(real)):
        topSum += (AI[i] - real[i])**2

        bottomSum += (real[i] - realMean)**2

    COE = 1 - (topSum / bottomSum)

    return COE


#Apply weights to each input value for each hidden layer and get their values
#Loop through data set passing each data value into this
def feedForward(input_layer):
    global newA
    newA = np.zeros(H_dim)

    for x in range(len(weights)):
        #Weighted Sum
        a = np.matmul(weights[x], input_layer.transpose())

        #Add the biases
        a = np.add(a, hB[x])

        #Send value through activation for hidden node value
        a = activation(a[0])

        newA[x] = a

    outputNode = np.matmul(hW.transpose(), newA)

    return (int(outputNode + oB))

def main():
    #Matrix to store values after each epoch
    epochM = np.zeros(len(testData))

    #Send each layer of test Data through feed forward algorithm
    for x in range(0, len(testData)):
        epochM[x] = activation(feedForward(testData[x]))

    return costFunction(desiredOut, epochM)
    
print(main())









