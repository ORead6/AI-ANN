import numpy as np
import random

#Change to get dataset in format as shown
testData = np.matrix([[10.4,4.393,9.291,0,0,0,4], 
            [9.95,4.239,8.622,0,0,0.8,0],
            [9.46,4.124,8.057,0,0,0.8,0],
            [9.41,4.363,7.925,2.4,24.8,0.8,61.6],
            [26.3,11.962,58.704,11.2,5.6,33.6,111.2],
            [32.1,10.237,34.416,0,0,1.6,0.8]])

desiredOut = [26.1, 24.86, 23.6, 23.47, 60.7, 98.01]
#Changing between 1-0

def normaliseData(data, case):
#Assign realistic min and max values with the ranges of data
    maxVal = 100 #Look through data set and change this value appropriately
    minVal = 0

    #Normalise data set between 0-1 for compairson
    if (case == "pre"):
        for x in range(0, len(data)):
            data[x] = (data[x] - minVal) / (maxVal - minVal)

        data = np.matrix(data).transpose()
        return data
    
    #Taking data between 0-1 and changing to put through correct range of values
    elif (case == "post"):
        for x in range(0, len(data)):
            data[x] = (data[x] * (maxVal - minVal)) + minVal
        data = np.matrix(data).transpose()
        return data


#factor to shrink inputs by to avoid activation of 6 being nearly the same and as accurate as 600 (sigmoid graph)
inputFactor = 0

#Neurons is how long the test Data array is
I_dim = 7

#5 Neurons
H_dim = 5

#3D Array for weights per layer
global weights
weights = np.zeros(shape=(H_dim, len(testData)+1))

#Hidden Layer Bias
hB = np.zeros(H_dim)

#Last layer of weights:
global hW
hW = np.zeros(H_dim).transpose()

#output Bias
oB = 0

def initWeights():
    #Init weight values using Xavier weight Intialisation
    for x in range(0, len(weights)):
        for i in range(0, len(weights[x])):
            w = random.uniform(-(1/np.sqrt(I_dim)), (1/np.sqrt(I_dim)))
            weights[x][i] = w

    for i in range(0, len(hW)):
        w = random.uniform(-(1/np.sqrt(I_dim)), (1/np.sqrt(I_dim)))
        hW[i] = w

#Activation Function
def activation(x):
    active = "sigmoid"

    if active == "sigmoid":
        #Sigmoid
        result = 1 / (1 + np.exp(-x))
        return result
    
    if active == "RELU":
        #relu
        if (x) >= 0:
            return x
        else:
            return 0    

#Error Function
def costFunction(real, AI):
    #SSE
    SSE = 0
    for i in range(0, len(real)):
        SSE += (real[i] - AI[i])**  2

    MSE = SSE * (1/(len(real)))

    return MSE


#Apply weights to each input value for each hidden layer and get their values
#Loop through data set passing each data value into this
def feedForward(input_layer):
    global newA
    newA = np.zeros(H_dim)

    for x in range(len(weights)):
        #Weighted Sum
        a = np.matmul(weights[x], input_layer.transpose())

        #Add the biases
        a = np.add(a, hB[x])

        #Send value through activation for hidden node value
        a = activation(a[0])

        newA[x] = a

    outputNode = np.matmul(hW.transpose(), newA)

    return (float(outputNode + oB))

def main():
    #Matrix to store values after each epoch
    initWeights()
    epochM = np.zeros(len(testData))

    #Send each layer of test Data through feed forward algorithm
    for x in range(0, len(testData)):
        epochM[x] = activation(feedForward(testData[x]))

    cost_after_epoch = costFunction(normaliseData(desiredOut, "pre"), epochM)

    print(cost_after_epoch)
    print(normaliseData(epochM, "post"))
    
main()










